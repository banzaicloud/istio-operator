# Istio multi mesh federation

In multi-mesh environments, meshes that would be otherwise independent are loosely coupled together using **ServiceEntries** for configuration and a **common root CA** as a base for secure communication through **Istio ingress gateways using mTLS**. From a networking standpoint, this setup's only requirement is that its ingress gateways be reachable from one another.

Workloads in each cluster can access existing **local** services via their Kubernetes DNS suffix, e.g., `<name>.<namespace>.svc.cluster.local`, as per usual. To reach services in **remote** meshes, Istio includes a CoreDNS server that can be configured to handle service names in the form of `<name>.<namespace>.global`, thus calls from any cluster to `foo.foons.global` will resolve to the `foo` service in namespace `foons` on the mesh on which it's running.

Every service in a given mesh that needs to be accessed from a different mesh requires a `ServiceEntry` configuration in a remote mesh. The host used in the service entry should take the form `<name>.<namespace>.global`, where `name` and `namespace` correspond to the service’s name and namespace respectively.

# Multi mesh federation example

For demonstrative purposes, create 2 clusters, a 2 node [Banzai Cloud PKE](https://banzaicloud.com/blog/pke-cncf-certified-k8s/) cluster on EC2 and GKE cluster with 2 nodes as well. That setup show that this solutions works not just in a multi-cluster, but also in a multi-cloud environment.

## Get the latest version of the [Istio operator](https://github.com/banzaicloud/istio-operator)

```bash
❯ git clone https://github.com/banzaicloud/istio-operator.git
❯ cd istio-operator
❯ git checkout release-1.6
```

## Create the clusters on the Banzai Cloud Pipeline platform

The [Pipeline platform](https://beta.banzaicloud.io/) is the easiest way to setup the demo environment via our [CLI tool](https://banzaicloud.com/blog/cli-ux/) ([install](https://github.com/banzaicloud/banzai-cli#installation)) for [Pipeline](https:/github.com/banzaicloud/pipeline), which is simply called `banzai`.

```bash
AWS_SECRET_ID="[[secretID from Pipeline]]"
GKE_SECRET_ID="[[secretID from Pipeline]]"
GKE_PROJECT_ID="<GKE project ID>"

❯ cat docs/federation/multimesh/istio-pke-cluster.json | sed "s/{{secretID}}/${AWS_SECRET_ID}/" | banzai cluster create
INFO[0004] cluster is being created
INFO[0004] you can check its status with the command `banzai cluster get "istio-multimesh-pke"`
Id   Name
741  istio-multimesh-pke

❯ cat docs/federation/multimesh/istio-gke-cluster.json | sed -e "s/{{secretID}}/${GKE_SECRET_ID}/" -e "s/{{projectID}}/${GKE_PROJECT_ID}/" | banzai cluster create
INFO[0005] cluster is being created
INFO[0005] you can check its status with the command `banzai cluster get "istio-gke"`
Id   Name
742  istio-gke
```

## Wait for the clusters to be up and running

```bash
❯ banzai cluster list
Id   Name                Distribution  Status   CreatorName    CreatedAt
742  istio-multimesh-gke   gke           RUNNING  waynz0r        2019-05-28T12:44:38Z
741  istio-multimesh-pke   pke           RUNNING  waynz0r        2019-05-28T12:38:45Z
```

Download the kubeconfigs from the [Pipeline UI](https://beta.banzaicloud.io) and set them as k8s contexts.

```bash
❯ export KUBECONFIG=~/Downloads/istio-multimesh-pke.yaml:~/Downloads/istio-multimesh-gke.yaml

❯ kubectl config get-contexts -o name
istio-multimesh-gke
kubernetes-admin@istio-pke

❯ export CTX_GKE=istio-multimesh-gke
❯ export CTX_PKE=kubernetes-admin@istio-multimesh-pke
```

## Setup PKE cluster

### Install the operator onto the PKE cluster

The following commands will deploy the operator to the `istio-system` namespace.

```bash
❯ kubectl config use-context ${CTX_PKE}
❯ make deploy
```

### Setup root CA and deploy Istio control plane on the PKE cluster

> Cross mesh communication requires mutual TLS connection between services. To enable mutual TLS communication across meshes, each mesh Citadel must be configured with intermediate CA credentials generated by a shared root CA. For demo purposes a sample root CA certificate is used.

The following commands will add the sample root CA certs as a secret. It also creates a custom resource definition in the cluster,
following a pattern typical of operators, this will allow you to specify your Istio configurations to a Kubernetes custom resource.
Once you apply that to your cluster, the operator will start reconciling the Istio components.

```bash
❯ kubectl create secret generic cacerts -n istio-system \
    --from-file=docs/federation/multimesh/certs/ca-cert.pem \
    --from-file=docs/federation/multimesh/certs/ca-key.pem \
    --from-file=docs/federation/multimesh/certs/root-cert.pem \
    --from-file=docs/federation/multimesh/certs/cert-chain.pem
❯ kubectl --context=${CTX_PKE} -n istio-system create -f docs/federation/multimesh/istio-multimesh-cr.yaml
```

Wait for the `multimesh` Istio resource status to become `Available` and for the pods in the `istio-system` to become ready.

```bash
❯ kubectl --context=${CTX_PKE} -n istio-system get istios
NAME        STATUS      ERROR   GATEWAYS           AGE
multimesh   Available           [35.180.106.193]   4m15s

❯ kubectl --context=${CTX_PKE} -n istio-system get pods
NAME                                      READY   STATUS    RESTARTS   AGE
istio-citadel-58c77cc58b-mj7tg            1/1     Running   0          4m12s
istio-egressgateway-6958db94bc-78dl7      1/1     Running   0          4m10s
istio-galley-5dd459c899-llt2k             1/1     Running   0          4m11s
istio-ingressgateway-7ddbbddc9f-dj9ls     1/1     Running   0          4m10s
istio-pilot-6b97586d79-lr9sz              2/2     Running   0          4m11s
istio-policy-8b7bd457-j5n59               2/2     Running   2          4m9s
istio-sidecar-injector-54d7d74bdb-mw4kn   1/1     Running   0          3m58s
istio-telemetry-86f6459cd5-mgvtv          2/2     Running   2          4m9s
istiocoredns-74dd777b79-z7nbp             2/2     Running   0          3m58s
```

## Setup GKE cluster

It takes exactly the same steps to setup the GKE cluster as well.

### Install the operator onto the GKE cluster

```bash
❯ kubectl config use-context ${CTX_GKE}
❯ make deploy
```

### Setup root CA and deploy Istio control plane on the GKE cluster

```bash
❯ kubectl create secret generic cacerts -n istio-system \
    --from-file=docs/federation/multimesh/certs/ca-cert.pem \
    --from-file=docs/federation/multimesh/certs/ca-key.pem \
    --from-file=docs/federation/multimesh/certs/root-cert.pem \
    --from-file=docs/federation/multimesh/certs/cert-chain.pem
❯ kubectl --context=${CTX_GKE} -n istio-system create -f docs/federation/multimesh/istio-multimesh-cr.yaml
```

Wait for the `multimesh` Istio resource status to become `Available` and for the pods in the `istio-system` to become ready.

```bash
❯ kubectl --context=${CTX_GKE} -n istio-system get istios
NAME        STATUS      ERROR   GATEWAYS           AGE
multimesh   Available           [35.180.106.193]   4m15s

❯ kubectl --context=${CTX_GKE} -n istio-system get pods
NAME                                      READY   STATUS    RESTARTS   AGE
istio-citadel-58c77cc58b-mj7tg            1/1     Running   0          4m12s
istio-egressgateway-6958db94bc-78dl7      1/1     Running   0          4m10s
istio-galley-5dd459c899-llt2k             1/1     Running   0          4m11s
istio-ingressgateway-7ddbbddc9f-dj9ls     1/1     Running   0          4m10s
istio-pilot-6b97586d79-lr9sz              2/2     Running   0          4m11s
istio-policy-8b7bd457-j5n59               2/2     Running   2          4m9s
istio-sidecar-injector-54d7d74bdb-mw4kn   1/1     Running   0          3m58s
istio-telemetry-86f6459cd5-mgvtv          2/2     Running   2          4m9s
istiocoredns-74dd777b79-z7nbp             2/2     Running   0          3m58s
```

## Testing, testing

Create a simple `echo` service on both clusters for testing purposes.

> Create `Gateway` and `VirtualService` resources as well to be able to reach the service through the ingress gateway.

```bash
❯ kubectl --context ${CTX_PKE} -n default apply -f docs/federation/multimesh/echo-service.yaml
❯ kubectl --context ${CTX_PKE} -n default apply -f docs/federation/multimesh/echo-gw.yaml
❯ kubectl --context ${CTX_PKE} -n default apply -f docs/federation/multimesh/echo-vs.yaml

❯ kubectl --context ${CTX_PKE} -n default get pods
NAME                    READY   STATUS    RESTARTS   AGE
echo-5c7dd5494d-k8nn9   2/2     Running   0          1m

❯ kubectl --context ${CTX_GKE} -n default apply -f docs/federation/multimesh/echo-service.yaml
❯ kubectl --context ${CTX_GKE} -n default apply -f docs/federation/multimesh/echo-gw.yaml
❯ kubectl --context ${CTX_GKE} -n default apply -f docs/federation/multimesh/echo-vs.yaml

❯ kubectl --context ${CTX_GKE} -n default get pods
NAME                    READY   STATUS    RESTARTS   AGE
echo-595496dfcc-6tpk5   2/2     Running   0          1m
```

Hit the PKE cluster's ingress with some traffic to see how the `echo` service responds:

```bash
❯ export PKE_INGRESS=$(kubectl --context=${CTX_PKE} -n istio-system get svc/istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

❯ for i in `seq 1 100`; do curl -s "http://${PKE_INGRESS}/" |grep "Hostname"; done | sort | uniq -c
 100 Hostname: echo-5c7dd5494d-k8nn9
```

So far so good, the only running pod in the echo service answered to every requests.

### Create a service entry in the PKE cluster for the `echo` service running on the GKE cluster

In order to allow access to `echo` running on the GKE cluster, we need to create a service entry for it in the PKE cluster. The host name of the service entry should be of the form `<name>.<namespace>.global` where name and namespace correspond to the remote service’s name and namespace respectively.

For DNS resolution for services under the `*.global` domain, you need to assign these services an IP address. In this example we’ll use IPs in 127.255.0.0/16. Application traffic for these IPs will be captured by the sidecar and routed to the appropriate remote service.

> Each service (in the .global DNS domain) must have a unique IP within the cluster, but they are not need to be routable.

```bash
❯ kubectl apply --context=$CTX_PKE -n default -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: echo-svc
spec:
  hosts:
  # must be of form name.namespace.global
  - echo.default.global
  location: MESH_INTERNAL
  ports:
  - name: http1
    number: 80
    protocol: http
  resolution: DNS
  addresses:
  - 127.255.0.1
  endpoints:
  - address: $(kubectl --context ${CTX_GKE} -n istio-system get svc/istio-ingressgateway -o jsonpath='{@.status.loadBalancer.ingress[0].ip}')
    ports:
      http1: 15443 # Do not change this port value
EOF
```

The configurations above will result that all traffic from the PKE cluster for `echo.default.global` to be routed to the endpoint `IPofGKEIngressGateway:15443` over a mutual TLS connection. The port 15443 for the ingress gateway is configured in a special SNI-aware `Gateway` resource that the operator installed as part of the reconciliation logic.

### Apply a revised VirtualService resource

The revised `VirtualService` is configured so that the traffic for `echo` service will be split 50/50 between endpoints in the two clusters.

```bash
❯ kubectl apply --context=$CTX_PKE -n default -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: echo
  namespace: default
spec:
  hosts:
  - "*"
  gateways:
  - echo-gateway.default.svc.cluster.local
  http:
  - route:
    - destination:
        host: echo.default.svc.cluster.local
        port:
          number: 80
      weight: 50
    - destination:
        host: echo.default.global
        port:
          number: 80
      weight: 50
EOF
```

Hit the PKE cluster's ingress again with some traffic:

```bash
❯ for i in `seq 1 100`; do curl -s "http://${PKE_INGRESS}/" |grep "Hostname"; done | sort | uniq -c
  45 Hostname: echo-595496dfcc-6tpk5
  55 Hostname: echo-5c7dd5494d-k8nn9
```

It's clear from the results that although we hit the PKE cluster's ingress gateway only, pods on the two clusters responded evenly.

### Cleanup

Execute the following commands to clean up the clusters:

```bash
❯ kubectl --context=${CTX_PKE} -n istio-system delete istios multimesh
❯ kubectl --context=${CTX_PKE} delete namespace istio-system

❯ kubectl --context=${CTX_GKE} -n istio-system delete istios multimesh
❯ kubectl --context=${CTX_GKE} delete namespace istio-system

❯ banzai cluster delete istio-multimesh-pke --no-interactive
❯ banzai cluster delete istio-multimesh-gke --no-interactive
